# -*- coding: utf-8 -*-
"""CMSC421 Project Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7A5v270VJnFoaiSTWKU1Vm4406sAAeL

Other resources:
https://www.kaggle.com/code/zabihullah18/email-spam-detection

# Imports and Data Exploration
"""

# import packages
# https://www.youtube.com/watch?v=2sXAYoPIz3A
# https://towardsdatascience.com/na%C3%AFve-bayes-spam-filter-from-scratch-12970ad3dae7
# https://github.com/makispl/SMS-Spam-Filter-Naive-Bayes/blob/master/SMS_Spam_Filtering_Naive_Bayes.ipynb
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_validate
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Importing NLTK for natural language processing
import nltk
from nltk.corpus import stopwords    # For stopwords
from nltk.stem.porter import PorterStemmer
import string

# Downloading NLTK data
nltk.download('stopwords')   # Downloading stopwords data
nltk.download('punkt')       # Downloading tokenizer data

# Importing WordCloud for text visualization
from wordcloud import WordCloud

import matplotlib.pyplot as plt  # For data visualization

# import data from https://www.kaggle.com/datasets/mfaisalqureshi/spam-email?resource=download
spam_df = pd.read_csv("spam.csv")

spam_df.head()

spam_df.groupby('Category').describe()

fig1, ax1 = plt.subplots(figsize=(5,5))

labels = ['Spam', 'Ham']
sizes = [len(spam_df[spam_df['Category'] == 'spam']), len(spam_df[spam_df['Category'] == 'ham'])]
explode = (0, 0.1)

ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90)
ax1.axis('equal')
ax1.set_title("SMS Data Set", fontsize=14)

plt.show()

spam_df['Category'] = spam_df['Category'].apply(lambda x: 1 if x == 'spam' else 0)

# Data Preprocessing

# Initialize NLTK's stopwords and Porter Stemmer
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
porter_stemmer = PorterStemmer()

# Function for text preprocessing
def preprocess_text(text):
    text = text.lower()
    tokens = nltk.word_tokenize(text)

    # Remove special characters, stopwords, and punctuation
    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words and token not in string.punctuation]

    stemmed_tokens = [porter_stemmer.stem(token) for token in filtered_tokens]
    processed_text = ' '.join(stemmed_tokens)

    return processed_text

# Example usage
example_text = 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'
processed_example_text = preprocess_text(example_text)
print(processed_example_text)

#x_train.describe()

#x_train_count = cv.fit_transform(x_train.values)

#x_train_count.toarray()

# @title Buidling Models
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split

cv = CountVectorizer()
tfid = TfidfVectorizer(max_features = 3000)

x = tfid.fit_transform(spam_df['Transformed Message']).toarray()
y = spam_df['Category'].values

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
# Below are extra models we can train
#from sklearn.ensemble import AdaBoostClassifier
#from sklearn.ensemble import BaggingClassifier
#from sklearn.ensemble import ExtraTreesClassifier
#from sklearn.ensemble import GradientBoostingClassifier
#from xgboost import XGBClassifier

x_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.20, random_state = 2)
classifiers = {"LR": LogisticRegression(solver="liblinear", penalty="l1"),
               "SVC": SVC(kernel= "sigmoid", gamma  = 1.0),
               "NB": MultinomialNB(),
               "DT": DecisionTreeClassifier(max_depth = 5),
               "KNN": KNeighborsClassifier(),
               "RF": RandomForestClassifier(n_estimators = 50, random_state = 2)}

from sklearn.metrics import accuracy_score, precision_score
def train_classifier(clfs, X_train, y_train, X_test, y_test):
    clfs.fit(X_train,y_train)
    y_pred = clfs.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    return accuracy , precision

# @title Evaluating Models
for name, classifiers in classifiers.items():
    current_accuracy, current_precision = train_classifier(classifiers, x_train, y_train, x_test, y_test)
    print("\nFor: ", name)
    print("Accuracy: ", current_accuracy)
    print("Precision: ", current_precision)
